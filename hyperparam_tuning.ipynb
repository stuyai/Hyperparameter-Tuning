{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbf9EEMfpmkI"
      },
      "outputs": [],
      "source": [
        "!pip install -q optuna torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QrClZaXprrN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
        "\n",
        "# Download MNIST\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('.', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split train into train (90%) and val (10%)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A95a_p0rpsfX"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_filters, dropout_rate):\n",
        "        super(CNN, self).__init__()\n",
        "        # First convolutional layer followed by batch normalization\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, 3)\n",
        "        self.bn1 = nn.BatchNorm2d(num_filters)\n",
        "        # Second convolutional layer followed by batch normalization\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, 3)\n",
        "        self.bn2 = nn.BatchNorm2d(num_filters * 2)\n",
        "        # Dropout layer for convolutional features\n",
        "        self.dropout2d = nn.Dropout2d(dropout_rate)\n",
        "        # Fully connected layers with dropout in between\n",
        "        self.fc1 = nn.Linear(num_filters * 2 * 5 * 5, 128)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout2d(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout2d(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example training (outside of the hyperparameter tuning) for clarity\n",
        "lr = 1e-2\n",
        "num_filters = 8\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "dropout_rate = 0.3\n",
        "\n",
        "# Create data loaders (assumes train_dataset and val_dataset are defined)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize model, optimizer with weight decay, and loss criterion\n",
        "model = CNN(num_filters, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop example\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(data), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            pred = model(data).argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += data.size(0)\n",
        "    val_acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "wl-I2AkrZDNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23b0bde-1de6-4c80-a975-3ee3bdc66213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Validation Accuracy: 0.9622\n",
            "Epoch 2/5, Validation Accuracy: 0.9702\n",
            "Epoch 3/5, Validation Accuracy: 0.9702\n",
            "Epoch 4/5, Validation Accuracy: 0.9712\n",
            "Epoch 5/5, Validation Accuracy: 0.9717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mpcz8WapwmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ffc7e2-82d0-4cd0-f736-f230a0fe8b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-05 20:25:03,643] A new study created in memory with name: no-name-81663264-5e37-443a-bbfc-250ce8b10ed4\n",
            "<ipython-input-16-ced47b57fe92>:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
            "<ipython-input-16-ced47b57fe92>:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
            "<ipython-input-16-ced47b57fe92>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
            "[I 2025-03-05 20:25:38,469] Trial 0 finished with value: 0.9883333333333333 and parameters: {'lr': 0.002111108171497272, 'num_filters': 32, 'batch_size': 128, 'dropout_rate': 0.40961533341663103, 'weight_decay': 0.00010614944708240787}. Best is trial 0 with value: 0.9883333333333333.\n",
            "[I 2025-03-05 20:26:19,697] Trial 1 finished with value: 0.9826666666666667 and parameters: {'lr': 0.005522835642490142, 'num_filters': 16, 'batch_size': 64, 'dropout_rate': 0.3189620781678171, 'weight_decay': 0.00023986276078317344}. Best is trial 0 with value: 0.9883333333333333.\n",
            "[I 2025-03-05 20:26:33,431] Trial 2 pruned. \n",
            "[I 2025-03-05 20:26:47,235] Trial 3 pruned. \n",
            "[I 2025-03-05 20:27:33,478] Trial 4 finished with value: 0.9891666666666666 and parameters: {'lr': 0.000636976373301173, 'num_filters': 32, 'batch_size': 64, 'dropout_rate': 0.33792793406041755, 'weight_decay': 2.855124114411267e-06}. Best is trial 4 with value: 0.9891666666666666.\n",
            "[I 2025-03-05 20:27:48,161] Trial 5 pruned. \n",
            "[I 2025-03-05 20:28:06,796] Trial 6 pruned. \n",
            "[I 2025-03-05 20:28:23,975] Trial 7 pruned. \n",
            "[I 2025-03-05 20:28:41,384] Trial 8 pruned. \n",
            "[I 2025-03-05 20:28:58,713] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial: {'lr': 0.000636976373301173, 'num_filters': 32, 'batch_size': 64, 'dropout_rate': 0.33792793406041755, 'weight_decay': 2.855124114411267e-06}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "\n",
        "# Hyperparameter tuning with Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters including dropout rate and weight decay\n",
        "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
        "    num_filters = trial.suggest_categorical('num_filters', [16, 24, 32])\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
        "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
        "    epochs = 5\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = CNN(num_filters, dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(data), target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                pred = model(data).argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += data.size(0)\n",
        "        val_acc = correct / total\n",
        "        trial.report(val_acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    return val_acc\n",
        "\n",
        "pruner = optuna.pruners.SuccessiveHalvingPruner()\n",
        "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(\"Best trial:\", study.best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t42WJhvZpyZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae19483d-2ef2-499d-fc1f-87014339d450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.989\n"
          ]
        }
      ],
      "source": [
        "# Combine train and validation sets\n",
        "full_train_dataset = ConcatDataset([train_dataset, val_dataset])\n",
        "batch_size = study.best_trial.params['batch_size']\n",
        "train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "best_params = study.best_trial.params\n",
        "\n",
        "# Pass both num_filters and dropout_rate to the model\n",
        "model = CNN(best_params['num_filters'], best_params['dropout_rate']).to(device)\n",
        "\n",
        "# Use the best learning rate and weight decay from the tuning study\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=best_params['lr'],\n",
        "    weight_decay=best_params['weight_decay']\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(data), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        pred = model(data).argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += data.size(0)\n",
        "print(\"Test accuracy:\", correct / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MpZG1-Cwp0XY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}